{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710c44a3-435b-45be-a3be-f9831724789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0201a815-e1c1-4608-9c92-881862c07bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57732d15-e498-4ef2-bc8f-92809c800e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d5e35c-c4e5-4a77-b7bd-42ef1f0a4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mayur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9107b48a-9c1f-4ebe-b574-f781e50cf6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/mayur/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\") #tokenizer for sentence n word tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579d0a5e-ab8b-4e41-8d3b-36a101af58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    tokens=word_tokenize(text.lower())\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if word.isalpha() and word not in stop_words and word not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7563456b-71aa-445f-9ddf-cca0c23fee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text for training the CBOW model\n",
    "text = \"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a7912e-63e7-45f1-9976-b2b9b23ece61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep', 'learning', 'known', 'deep']\n",
      "also\n",
      "*******************\n",
      "['learning', 'also', 'deep', 'structured']\n",
      "known\n",
      "*******************\n",
      "['also', 'known', 'structured', 'learning']\n",
      "deep\n",
      "*******************\n",
      "['known', 'deep', 'learning', 'part']\n",
      "structured\n",
      "*******************\n",
      "['deep', 'structured', 'part', 'broader']\n",
      "learning\n",
      "*******************\n",
      "['structured', 'learning', 'broader', 'family']\n",
      "part\n",
      "*******************\n",
      "['learning', 'part', 'family', 'machine']\n",
      "broader\n",
      "*******************\n",
      "['part', 'broader', 'machine', 'learning']\n",
      "family\n",
      "*******************\n",
      "['broader', 'family', 'learning', 'methods']\n",
      "machine\n",
      "*******************\n",
      "['family', 'machine', 'methods', 'based']\n",
      "learning\n",
      "*******************\n",
      "['machine', 'learning', 'based', 'artificial']\n",
      "methods\n",
      "*******************\n",
      "['learning', 'methods', 'artificial', 'neural']\n",
      "based\n",
      "*******************\n",
      "['methods', 'based', 'neural', 'networks']\n",
      "artificial\n",
      "*******************\n",
      "['based', 'artificial', 'networks', 'representation']\n",
      "neural\n",
      "*******************\n",
      "['artificial', 'neural', 'representation', 'learning']\n",
      "networks\n",
      "*******************\n",
      "['neural', 'networks', 'learning', 'learning']\n",
      "representation\n",
      "*******************\n",
      "['networks', 'representation', 'learning', 'supervised']\n",
      "learning\n",
      "*******************\n",
      "['representation', 'learning', 'supervised', 'unsupervised']\n",
      "learning\n",
      "*******************\n",
      "['learning', 'learning', 'unsupervised', 'architectures']\n",
      "supervised\n",
      "*******************\n",
      "['learning', 'supervised', 'architectures', 'deep']\n",
      "unsupervised\n",
      "*******************\n",
      "['supervised', 'unsupervised', 'deep', 'neural']\n",
      "architectures\n",
      "*******************\n",
      "['unsupervised', 'architectures', 'neural', 'networks']\n",
      "deep\n",
      "*******************\n",
      "['architectures', 'deep', 'networks', 'deep']\n",
      "neural\n",
      "*******************\n",
      "['deep', 'neural', 'deep', 'belief']\n",
      "networks\n",
      "*******************\n",
      "['neural', 'networks', 'belief', 'networks']\n",
      "deep\n",
      "*******************\n",
      "['networks', 'deep', 'networks', 'deep']\n",
      "belief\n",
      "*******************\n",
      "['deep', 'belief', 'deep', 'reinforcement']\n",
      "networks\n",
      "*******************\n",
      "['belief', 'networks', 'reinforcement', 'learning']\n",
      "deep\n",
      "*******************\n",
      "['networks', 'deep', 'learning', 'recurrent']\n",
      "reinforcement\n",
      "*******************\n",
      "['deep', 'reinforcement', 'recurrent', 'neural']\n",
      "learning\n",
      "*******************\n",
      "['reinforcement', 'learning', 'neural', 'networks']\n",
      "recurrent\n",
      "*******************\n",
      "['learning', 'recurrent', 'networks', 'convolutional']\n",
      "neural\n",
      "*******************\n",
      "['recurrent', 'neural', 'convolutional', 'neural']\n",
      "networks\n",
      "*******************\n",
      "['neural', 'networks', 'neural', 'networks']\n",
      "convolutional\n",
      "*******************\n",
      "['networks', 'convolutional', 'networks', 'transformers']\n",
      "neural\n",
      "*******************\n",
      "['convolutional', 'neural', 'transformers', 'applied']\n",
      "networks\n",
      "*******************\n",
      "['neural', 'networks', 'applied', 'fields']\n",
      "transformers\n",
      "*******************\n",
      "['networks', 'transformers', 'fields', 'including']\n",
      "applied\n",
      "*******************\n",
      "['transformers', 'applied', 'including', 'computer']\n",
      "fields\n",
      "*******************\n",
      "['applied', 'fields', 'computer', 'vision']\n",
      "including\n",
      "*******************\n",
      "['fields', 'including', 'vision', 'speech']\n",
      "computer\n",
      "*******************\n",
      "['including', 'computer', 'speech', 'recognition']\n",
      "vision\n",
      "*******************\n",
      "['computer', 'vision', 'recognition', 'natural']\n",
      "speech\n",
      "*******************\n",
      "['vision', 'speech', 'natural', 'language']\n",
      "recognition\n",
      "*******************\n",
      "['speech', 'recognition', 'language', 'processing']\n",
      "natural\n",
      "*******************\n",
      "['recognition', 'natural', 'processing', 'machine']\n",
      "language\n",
      "*******************\n",
      "['natural', 'language', 'machine', 'translation']\n",
      "processing\n",
      "*******************\n",
      "['language', 'processing', 'translation', 'bioinformatics']\n",
      "machine\n",
      "*******************\n",
      "['processing', 'machine', 'bioinformatics', 'drug']\n",
      "translation\n",
      "*******************\n",
      "['machine', 'translation', 'drug', 'design']\n",
      "bioinformatics\n",
      "*******************\n",
      "['translation', 'bioinformatics', 'design', 'medical']\n",
      "drug\n",
      "*******************\n",
      "['bioinformatics', 'drug', 'medical', 'image']\n",
      "design\n",
      "*******************\n",
      "['drug', 'design', 'image', 'analysis']\n",
      "medical\n",
      "*******************\n",
      "['design', 'medical', 'analysis', 'climate']\n",
      "image\n",
      "*******************\n",
      "['medical', 'image', 'climate', 'science']\n",
      "analysis\n",
      "*******************\n",
      "['image', 'analysis', 'science', 'material']\n",
      "climate\n",
      "*******************\n",
      "['analysis', 'climate', 'material', 'inspection']\n",
      "science\n",
      "*******************\n",
      "['climate', 'science', 'inspection', 'board']\n",
      "material\n",
      "*******************\n",
      "['science', 'material', 'board', 'game']\n",
      "inspection\n",
      "*******************\n",
      "['material', 'inspection', 'game', 'programs']\n",
      "board\n",
      "*******************\n",
      "['inspection', 'board', 'programs', 'produced']\n",
      "game\n",
      "*******************\n",
      "['board', 'game', 'produced', 'results']\n",
      "programs\n",
      "*******************\n",
      "['game', 'programs', 'results', 'comparable']\n",
      "produced\n",
      "*******************\n",
      "['programs', 'produced', 'comparable', 'cases']\n",
      "results\n",
      "*******************\n",
      "['produced', 'results', 'cases', 'surpassing']\n",
      "comparable\n",
      "*******************\n",
      "['results', 'comparable', 'surpassing', 'human']\n",
      "cases\n",
      "*******************\n",
      "['comparable', 'cases', 'human', 'expert']\n",
      "surpassing\n",
      "*******************\n",
      "['cases', 'surpassing', 'expert', 'performance']\n",
      "human\n",
      "*******************\n",
      "[['deep', 'learning', 'known', 'deep', 'also'], ['learning', 'also', 'deep', 'structured', 'known'], ['also', 'known', 'structured', 'learning', 'deep'], ['known', 'deep', 'learning', 'part', 'structured'], ['deep', 'structured', 'part', 'broader', 'learning'], ['structured', 'learning', 'broader', 'family', 'part'], ['learning', 'part', 'family', 'machine', 'broader'], ['part', 'broader', 'machine', 'learning', 'family'], ['broader', 'family', 'learning', 'methods', 'machine'], ['family', 'machine', 'methods', 'based', 'learning'], ['machine', 'learning', 'based', 'artificial', 'methods'], ['learning', 'methods', 'artificial', 'neural', 'based'], ['methods', 'based', 'neural', 'networks', 'artificial'], ['based', 'artificial', 'networks', 'representation', 'neural'], ['artificial', 'neural', 'representation', 'learning', 'networks'], ['neural', 'networks', 'learning', 'learning', 'representation'], ['networks', 'representation', 'learning', 'supervised', 'learning'], ['representation', 'learning', 'supervised', 'unsupervised', 'learning'], ['learning', 'learning', 'unsupervised', 'architectures', 'supervised'], ['learning', 'supervised', 'architectures', 'deep', 'unsupervised'], ['supervised', 'unsupervised', 'deep', 'neural', 'architectures'], ['unsupervised', 'architectures', 'neural', 'networks', 'deep'], ['architectures', 'deep', 'networks', 'deep', 'neural'], ['deep', 'neural', 'deep', 'belief', 'networks'], ['neural', 'networks', 'belief', 'networks', 'deep'], ['networks', 'deep', 'networks', 'deep', 'belief'], ['deep', 'belief', 'deep', 'reinforcement', 'networks'], ['belief', 'networks', 'reinforcement', 'learning', 'deep'], ['networks', 'deep', 'learning', 'recurrent', 'reinforcement'], ['deep', 'reinforcement', 'recurrent', 'neural', 'learning'], ['reinforcement', 'learning', 'neural', 'networks', 'recurrent'], ['learning', 'recurrent', 'networks', 'convolutional', 'neural'], ['recurrent', 'neural', 'convolutional', 'neural', 'networks'], ['neural', 'networks', 'neural', 'networks', 'convolutional'], ['networks', 'convolutional', 'networks', 'transformers', 'neural'], ['convolutional', 'neural', 'transformers', 'applied', 'networks'], ['neural', 'networks', 'applied', 'fields', 'transformers'], ['networks', 'transformers', 'fields', 'including', 'applied'], ['transformers', 'applied', 'including', 'computer', 'fields'], ['applied', 'fields', 'computer', 'vision', 'including'], ['fields', 'including', 'vision', 'speech', 'computer'], ['including', 'computer', 'speech', 'recognition', 'vision'], ['computer', 'vision', 'recognition', 'natural', 'speech'], ['vision', 'speech', 'natural', 'language', 'recognition'], ['speech', 'recognition', 'language', 'processing', 'natural'], ['recognition', 'natural', 'processing', 'machine', 'language'], ['natural', 'language', 'machine', 'translation', 'processing'], ['language', 'processing', 'translation', 'bioinformatics', 'machine'], ['processing', 'machine', 'bioinformatics', 'drug', 'translation'], ['machine', 'translation', 'drug', 'design', 'bioinformatics'], ['translation', 'bioinformatics', 'design', 'medical', 'drug'], ['bioinformatics', 'drug', 'medical', 'image', 'design'], ['drug', 'design', 'image', 'analysis', 'medical'], ['design', 'medical', 'analysis', 'climate', 'image'], ['medical', 'image', 'climate', 'science', 'analysis'], ['image', 'analysis', 'science', 'material', 'climate'], ['analysis', 'climate', 'material', 'inspection', 'science'], ['climate', 'science', 'inspection', 'board', 'material'], ['science', 'material', 'board', 'game', 'inspection'], ['material', 'inspection', 'game', 'programs', 'board'], ['inspection', 'board', 'programs', 'produced', 'game'], ['board', 'game', 'produced', 'results', 'programs'], ['game', 'programs', 'results', 'comparable', 'produced'], ['programs', 'produced', 'comparable', 'cases', 'results'], ['produced', 'results', 'cases', 'surpassing', 'comparable'], ['results', 'comparable', 'surpassing', 'human', 'cases'], ['comparable', 'cases', 'human', 'expert', 'surpassing'], ['cases', 'surpassing', 'expert', 'performance', 'human']]\n"
     ]
    }
   ],
   "source": [
    "tokens=pre_process(text)\n",
    "training_data=[]\n",
    "context_size=2\n",
    "for i in range (context_size , len(tokens)-context_size):\n",
    "    context=tokens[i-context_size:i]+tokens[i+1:i+1+context_size]\n",
    "    target=tokens[i]\n",
    "    training_data.append(context+[target])\n",
    "    print(context)\n",
    "    print(target)\n",
    "    print(\"*******************\")\n",
    "\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc62d9f-134b-4ca5-970f-473de0c3576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec(sentences=training_data , vector_size=500 , sg=0 , min_count=1 , workers=4, window=context_size)\n",
    "#word into vector with 500 dimensions,sg=0 cBOW method context se target sg=1 skip-gram mthod,min frequency of word,no.of worker thread to train model speed up training process,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e80a26c-ca05-40a8-a0e9-25a146924c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1901, 6800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.train(training_data , total_examples=len(training_data) , epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "867bd382-35f9-423e-b702-d44c53cce53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict():\n",
    "    i=random.randint(2 , len(tokens)-3)\n",
    "    prev_words=tokens[i-2:i]\n",
    "    next_words=tokens[i+1:i+3]\n",
    "    predict_word=model.predict_output_word(prev_words+next_words , topn=1)\n",
    "    print(f\"Context words are: {prev_words+next_words}\")\n",
    "    print(f\"target word is: {predict_word}\")\n",
    "\n",
    "    print(f\"Most similar words to : {predict_word}\")\n",
    "    similar_words=model.wv.most_similar(predict_word , topn=5)\n",
    "    for word , similarity in similar_words:\n",
    "        print(f\"{word}:{similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f2c306-7bda-48d8-b0c7-b86405f6194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words are: ['including', 'computer', 'speech', 'recognition']\n",
      "target word is: [('vision', 0.018182622)]\n",
      "Most similar words to : [('vision', 0.018182622)]\n",
      "image:0.14924746751785278\n",
      "networks:0.14555232226848602\n",
      "neural:0.13218741118907928\n",
      "recognition:0.09510860592126846\n",
      "deep:0.0935056284070015\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a46f2-6dc1-4c95-b7e4-ad3582bf2a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dca484-53a0-45c9-a6b9-e6bd1051c15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac9b99-8ce8-4ed5-a170-449f53c141d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cbd6f-3fca-4ea2-b912-9a130224ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0969823-a9e8-4129-9ef5-020e50466ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff629f-c63a-4ab1-89dd-b044c856fa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ba980-6dcf-4325-bcee-a6e59731ea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bbe3d-3dd6-428b-bf13-9e61365124e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f4314-7295-4ca7-9a0d-b5defdbf06fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
